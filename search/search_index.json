{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ds-cookiecutter guide (Kuebiko) \u00b6 Kuebiko (\u4e45\u5ef6\u6bd8\u53e4) is the Shinto kami (\"god; deity\") of Folk wisdom, knowledge and agriculture, and is represented in Japanese mythology as a scarecrow who cannot walk but has comprehensive awareness. A \"full\" (see Corners cut ) example ds-cookiecutter project broken down into episodes, with annotations/narrative for each episode focusing on a different aspect. In addition, there are a set of reference guides consolidating key concepts and pointing to further resources. Project problem statement \u00b6 You have been given a big list (>100,000) of URLs corresponding to the business websites of UK companies. The stakeholder is interested in better understanding the performance, accessibility, and legal compliance of business websites across different industries (SIC codes). Some specific questions they have are: It is a requirement that limited companies must display the company's registered number on their website, how many businesses actually list a company number? Does the download size, response time, or reachability of a business' website bear any relation to the number of web development businesses nearby? How do web development businesses themselves do on the extracted features mentioned above? Is there any relationship between aforementioned extracted features and text on a business' website relating to what a business does? Episode plan \u00b6 Below is a list of provisional episodes. See issues labeled episode for more information and more timely updates. [x] 0. Setup [ ] 1. Metaflow basics - fetching auxiliary data from the web [ ] 2. Getting data from SQL - fetching NOMIS auxiliary data [ ] 3. Critical coding - web-scraping utilities [ ] 4. Metaflow on AWS batch - scaling a web-scraper [ ] 5. Advanced data getters - extracting and explored scraped data [ ] 6. Data analysis - answering stakeholders first 3 questions [ ] 7. Perfect is the enemy of good - Company description extraction [ ] 8. Metaflow on AWS batch with a GPU - transformers [ ] 9. The fine line between analysis and pipeline - answering stakeholders final question [ ] 10. Refactoring Further episodes may be added as further use-cases arise, further guidelines are developed or new technologies adopted (e.g. AWS step-functions). How to read/use \u00b6 For each released episode the episode overview gives: a summary of what is tackled in the episode (both features but also associated guides); a description of the important new/modified files; and any important notes such as whether future episodes will improve upon specific aspects and whether an episode stands alone or builds upon a previous episode. For each episode there is a page at episodes/episode_XX which is the starting point for the episode. It will walk through the episode referring to files. Files with a * at the end are themselves annotated and you should read through these files and their annotations as the text refers to them. Normal python comments, e.g. # hello there , belong in a real (not an example) data science project; however comments with two hashes like ## hello there are for the narrative purposes of this guide. In the online documentation, narrative comments are rendered as markdown on the left hand side of the page and ordinary comments are kept within the rendered source. Note: If reading the raw source files or using the github UI then some links may not work as this repo is targeting the online docs first. Where links don't work, the link text should be informative enough to send you to the right place! Corners cut \u00b6 This is a MVP of a data science project following the ds-cookiecutter workflow but some corners have been cut: The problem statement is a bit contrived. It was chosen in order to demonstrate a variety of day-to-day aspects of data science development whilst keeping the overall level of content as small as possible to aid accessibility. We present and discuss complete, but not necessarily final, features - e.g. models work end-to-end but may only be simple. A detailed iterative workflow such as exploring in notebooks and then refactoring into modules, is not covered because: This would further bloat the already ambitious scope of this repository Everyone works differently and projects are nuanced therefore it does not pay to be opinonated here There are plans to record a video demoing and discussing one possible workflow Development/commit order is chosen for episodic release not necessarily the order which a real project would have Miscellaneous notes \u00b6 To avoid covering too much at once, early episodes are missing important things such as tests and data quality and data validation. These are covered a few episodes in and are retrospectively added at that point; however in a real project tests should always be written at the same time as the code they test (or even before if you are a proponent of test driven development)! When an episode gets opened for review then the code is only in a state/quality that any other PR might be at when a review is requested. I.e. there will be mistakes/shortcomings so it's up to DAP as a community to help improve things. The ds-cookiecutter template this is based on has not yet been merged (awaiting further reviews)","title":"Home"},{"location":"#ds-cookiecutter-guide-kuebiko","text":"Kuebiko (\u4e45\u5ef6\u6bd8\u53e4) is the Shinto kami (\"god; deity\") of Folk wisdom, knowledge and agriculture, and is represented in Japanese mythology as a scarecrow who cannot walk but has comprehensive awareness. A \"full\" (see Corners cut ) example ds-cookiecutter project broken down into episodes, with annotations/narrative for each episode focusing on a different aspect. In addition, there are a set of reference guides consolidating key concepts and pointing to further resources.","title":"ds-cookiecutter guide (Kuebiko)"},{"location":"#project-problem-statement","text":"You have been given a big list (>100,000) of URLs corresponding to the business websites of UK companies. The stakeholder is interested in better understanding the performance, accessibility, and legal compliance of business websites across different industries (SIC codes). Some specific questions they have are: It is a requirement that limited companies must display the company's registered number on their website, how many businesses actually list a company number? Does the download size, response time, or reachability of a business' website bear any relation to the number of web development businesses nearby? How do web development businesses themselves do on the extracted features mentioned above? Is there any relationship between aforementioned extracted features and text on a business' website relating to what a business does?","title":"Project problem statement"},{"location":"#episode-plan","text":"Below is a list of provisional episodes. See issues labeled episode for more information and more timely updates. [x] 0. Setup [ ] 1. Metaflow basics - fetching auxiliary data from the web [ ] 2. Getting data from SQL - fetching NOMIS auxiliary data [ ] 3. Critical coding - web-scraping utilities [ ] 4. Metaflow on AWS batch - scaling a web-scraper [ ] 5. Advanced data getters - extracting and explored scraped data [ ] 6. Data analysis - answering stakeholders first 3 questions [ ] 7. Perfect is the enemy of good - Company description extraction [ ] 8. Metaflow on AWS batch with a GPU - transformers [ ] 9. The fine line between analysis and pipeline - answering stakeholders final question [ ] 10. Refactoring Further episodes may be added as further use-cases arise, further guidelines are developed or new technologies adopted (e.g. AWS step-functions).","title":"Episode plan"},{"location":"#how-to-readuse","text":"For each released episode the episode overview gives: a summary of what is tackled in the episode (both features but also associated guides); a description of the important new/modified files; and any important notes such as whether future episodes will improve upon specific aspects and whether an episode stands alone or builds upon a previous episode. For each episode there is a page at episodes/episode_XX which is the starting point for the episode. It will walk through the episode referring to files. Files with a * at the end are themselves annotated and you should read through these files and their annotations as the text refers to them. Normal python comments, e.g. # hello there , belong in a real (not an example) data science project; however comments with two hashes like ## hello there are for the narrative purposes of this guide. In the online documentation, narrative comments are rendered as markdown on the left hand side of the page and ordinary comments are kept within the rendered source. Note: If reading the raw source files or using the github UI then some links may not work as this repo is targeting the online docs first. Where links don't work, the link text should be informative enough to send you to the right place!","title":"How to read/use"},{"location":"#corners-cut","text":"This is a MVP of a data science project following the ds-cookiecutter workflow but some corners have been cut: The problem statement is a bit contrived. It was chosen in order to demonstrate a variety of day-to-day aspects of data science development whilst keeping the overall level of content as small as possible to aid accessibility. We present and discuss complete, but not necessarily final, features - e.g. models work end-to-end but may only be simple. A detailed iterative workflow such as exploring in notebooks and then refactoring into modules, is not covered because: This would further bloat the already ambitious scope of this repository Everyone works differently and projects are nuanced therefore it does not pay to be opinonated here There are plans to record a video demoing and discussing one possible workflow Development/commit order is chosen for episodic release not necessarily the order which a real project would have","title":"Corners cut"},{"location":"#miscellaneous-notes","text":"To avoid covering too much at once, early episodes are missing important things such as tests and data quality and data validation. These are covered a few episodes in and are retrospectively added at that point; however in a real project tests should always be written at the same time as the code they test (or even before if you are a proponent of test driven development)! When an episode gets opened for review then the code is only in a state/quality that any other PR might be at when a review is requested. I.e. there will be mistakes/shortcomings so it's up to DAP as a community to help improve things. The ds-cookiecutter template this is based on has not yet been merged (awaiting further reviews)","title":"Miscellaneous notes"},{"location":"episodes/","text":"Episode overview \u00b6 Overview of released episodes content, see issues labeled episode for planned episodes. 1. Metaflow basics - fetching auxillary data from the web \u00b6 This episode walks through the basics of using Metaflow idiomatically - i.e. in a way that suits DAP and its use-cases - by fetching three auxilliary datasets needed: Companies House lookups to get the SIC code and address for each company number. National Statistics Postcode Lookup (NSPL) which will allow us to identify the Local Authority District (LADs) a company belongs to by matching its postcode. SIC taxonomy lookup between names and codes In addition, the first set of content (later episodes add more advanced content) is added to a Metaflow guide. Important notes \u00b6 There are missing pieces to this episode that a data-science PR should have. The most obvious of these is the absence of tests which are the subject of episode 3. Tests for the three pipelines of this episode will be added in the testing episode in order to keep the content of this episode focused around writing basic Metaflow flows and getters. Besides the episode guide there is neither documentation of how to run the flows or version-controlled configuration for flow parameters. This will be addressed in episode 4. Key files \u00b6 Episode 1 guide kuebiko/pipeline/sic/flow.py kuebiko/getters/sic.py kuebiko/pipeline/nspl/flow.py kuebiko/getters/nspl.py kuebiko/pipeline/companies_house/flow.py kuebiko/getters/companies_house.py Metaflow guide","title":"Episode overview"},{"location":"episodes/#episode-overview","text":"Overview of released episodes content, see issues labeled episode for planned episodes.","title":"Episode overview"},{"location":"episodes/#1-metaflow-basics-fetching-auxillary-data-from-the-web","text":"This episode walks through the basics of using Metaflow idiomatically - i.e. in a way that suits DAP and its use-cases - by fetching three auxilliary datasets needed: Companies House lookups to get the SIC code and address for each company number. National Statistics Postcode Lookup (NSPL) which will allow us to identify the Local Authority District (LADs) a company belongs to by matching its postcode. SIC taxonomy lookup between names and codes In addition, the first set of content (later episodes add more advanced content) is added to a Metaflow guide.","title":"1. Metaflow basics - fetching auxillary data from the web"},{"location":"episodes/#important-notes","text":"There are missing pieces to this episode that a data-science PR should have. The most obvious of these is the absence of tests which are the subject of episode 3. Tests for the three pipelines of this episode will be added in the testing episode in order to keep the content of this episode focused around writing basic Metaflow flows and getters. Besides the episode guide there is neither documentation of how to run the flows or version-controlled configuration for flow parameters. This will be addressed in episode 4.","title":"Important notes"},{"location":"episodes/#key-files","text":"Episode 1 guide kuebiko/pipeline/sic/flow.py kuebiko/getters/sic.py kuebiko/pipeline/nspl/flow.py kuebiko/getters/nspl.py kuebiko/pipeline/companies_house/flow.py kuebiko/getters/companies_house.py Metaflow guide","title":"Key files"},{"location":"episodes/episode_01/","text":"1. Metaflow basics - fetching auxillary data from the web \u00b6 This is not a Metaflow tutorial! It is assumed that you have already looked through the official Metaflow tutorials and familiarised yourself with the Metaflow docs . Standard Industrial Taxonomy (SIC) \u00b6 The first dataset that is fetched and processed is an Excel spreadsheet containing the codes and names of each of the 5 levels of the Standard Industrial Classification (SIC) taxonomy. The flow code can found in kuebiko/pipeline/sic/flow.py * and its utility functions (which are not a focus of this episode) in kuebiko/pipeline/sic/utils.py . Running python kuebiko/pipeline/sic/flow.py show will give us the docstrings and basic structure of the flow. Running python kuebiko/pipeline/sic/flow.py run will run the flow. Running python kuebiko/pipeline/sic/flow.py dump <run id>/end will dump a summary of the artifacts for a given run ID. Because .envrc specifies METAFLOW_DEFAULT_DATASTORE=local , data artifacts are persisted on your local machine (in a .metaflow/ folder), not to S3. To make the flow store artifacts in S3 then we run python kuebiko/pipeline/sic/flow.py --datastore=s3 run . kuebiko/getters/sic.py * implements a getter function to load the lookups from our flow. Writing a getter may seem to be an uneccessary level of indirection; however it is important to have this level of indirection because getters : Allows us to create derived views where Metaflow artifacts may not map 1:1 to getter functions Provides a place to go to discover what data is available for analysis along in a logical folder/file structure with clear doc-strings per function It is very important that getters relate to the problem domain! Enables the possibility of extra data validation before returning to the user National Statistics Postcode Lookup (NSPL) \u00b6 The second dataset that is fetched and processed is the National Statistics Postcode Lookup (NSPL), a lookup from UK postcodes to various statistical geographies. In this project we wish to extract the Local Authority District (variably referred to as LAD or LAUA), latitude, and longitude of each postcode. The Local Authority Districts will enable spatial aggregation of businesses to a level at which official statistics relating to the number of businesses in each SIC code in a given area. The latitude and longitude will enable other activities such as plotting. The flow code can found in kuebiko/pipeline/nspl/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/nspl/utils.py , and its getters in kuebiko/getters/nspl.py * Running python kuebiko/pipeline/nspl/flow.py --environment=conda run will run the flow. Note that we need to pass --environment=conda because this flow builds and executes in an isolated conda environment. Companies House \u00b6 The third dataset that is fetched and processed is the Companies House (UK company registrar) monthly data dump. Each registered UK company has a company number. By finding company numbers on websites we can match companies to Companies House obtaining information such as the SIC code they classify their activities as and their registered trading addresses. The flow code can found in kuebiko/pipeline/companies_house/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/companies_house/utils.py , and its getters in kuebiko/getters/companies_house.py * . Running python kuebiko/pipeline/companies_house/flow.py run will run the flow. In kuebiko/pipeline/sic/flow.py * we talked about the size of metaflow steps and the fact that persisting artifacts with the S3 datastore has some overhead, but how much? The Companies House dataset is quite large (several GB of RAM as a Pandas dataframe), so lets use this to explore the slowdown we might see close to the worst case: time python kuebiko/pipeline/companies_house/flow.py run --max-workers 1 ~10 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 1 ~26 minutes time python kuebiko/pipeline/companies_house/flow.py run --max-workers 4 ~4 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 4 ~19 minutes A slowdown of 2.5x (5x with more parallelism) is rather large, but do not despair: This was picked as a particularly bad case In reality, you only need to pay this cost once when your feature is complete and you run with --production --datastore=s3 . During testing/development you can use --datastore=local (or if you specify METAFLOW_DEFAULT_DATASTORE=local in .envrc then you don't need to explicitly choose this) If the flow was run on batch, which requires the S3 datastore, (covered in a later episode) then the download and upload speeds would be faster and the slowdown not so severe If all else fails you can always use the local datastore and use metaflow.S3 to save and share the necessary data at the end of the flow Note: You should use the self context, i.e. with S3(run=self) as s3: ... so that data is versioned under the current run ID and doesn't risk overwriting someone else's version of the data.","title":"01 - Metaflow basics"},{"location":"episodes/episode_01/#1-metaflow-basics-fetching-auxillary-data-from-the-web","text":"This is not a Metaflow tutorial! It is assumed that you have already looked through the official Metaflow tutorials and familiarised yourself with the Metaflow docs .","title":"1. Metaflow basics - fetching auxillary data from the web"},{"location":"episodes/episode_01/#standard-industrial-taxonomy-sic","text":"The first dataset that is fetched and processed is an Excel spreadsheet containing the codes and names of each of the 5 levels of the Standard Industrial Classification (SIC) taxonomy. The flow code can found in kuebiko/pipeline/sic/flow.py * and its utility functions (which are not a focus of this episode) in kuebiko/pipeline/sic/utils.py . Running python kuebiko/pipeline/sic/flow.py show will give us the docstrings and basic structure of the flow. Running python kuebiko/pipeline/sic/flow.py run will run the flow. Running python kuebiko/pipeline/sic/flow.py dump <run id>/end will dump a summary of the artifacts for a given run ID. Because .envrc specifies METAFLOW_DEFAULT_DATASTORE=local , data artifacts are persisted on your local machine (in a .metaflow/ folder), not to S3. To make the flow store artifacts in S3 then we run python kuebiko/pipeline/sic/flow.py --datastore=s3 run . kuebiko/getters/sic.py * implements a getter function to load the lookups from our flow. Writing a getter may seem to be an uneccessary level of indirection; however it is important to have this level of indirection because getters : Allows us to create derived views where Metaflow artifacts may not map 1:1 to getter functions Provides a place to go to discover what data is available for analysis along in a logical folder/file structure with clear doc-strings per function It is very important that getters relate to the problem domain! Enables the possibility of extra data validation before returning to the user","title":"Standard Industrial Taxonomy (SIC)"},{"location":"episodes/episode_01/#national-statistics-postcode-lookup-nspl","text":"The second dataset that is fetched and processed is the National Statistics Postcode Lookup (NSPL), a lookup from UK postcodes to various statistical geographies. In this project we wish to extract the Local Authority District (variably referred to as LAD or LAUA), latitude, and longitude of each postcode. The Local Authority Districts will enable spatial aggregation of businesses to a level at which official statistics relating to the number of businesses in each SIC code in a given area. The latitude and longitude will enable other activities such as plotting. The flow code can found in kuebiko/pipeline/nspl/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/nspl/utils.py , and its getters in kuebiko/getters/nspl.py * Running python kuebiko/pipeline/nspl/flow.py --environment=conda run will run the flow. Note that we need to pass --environment=conda because this flow builds and executes in an isolated conda environment.","title":"National Statistics Postcode Lookup (NSPL)"},{"location":"episodes/episode_01/#companies-house","text":"The third dataset that is fetched and processed is the Companies House (UK company registrar) monthly data dump. Each registered UK company has a company number. By finding company numbers on websites we can match companies to Companies House obtaining information such as the SIC code they classify their activities as and their registered trading addresses. The flow code can found in kuebiko/pipeline/companies_house/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/companies_house/utils.py , and its getters in kuebiko/getters/companies_house.py * . Running python kuebiko/pipeline/companies_house/flow.py run will run the flow. In kuebiko/pipeline/sic/flow.py * we talked about the size of metaflow steps and the fact that persisting artifacts with the S3 datastore has some overhead, but how much? The Companies House dataset is quite large (several GB of RAM as a Pandas dataframe), so lets use this to explore the slowdown we might see close to the worst case: time python kuebiko/pipeline/companies_house/flow.py run --max-workers 1 ~10 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 1 ~26 minutes time python kuebiko/pipeline/companies_house/flow.py run --max-workers 4 ~4 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 4 ~19 minutes A slowdown of 2.5x (5x with more parallelism) is rather large, but do not despair: This was picked as a particularly bad case In reality, you only need to pay this cost once when your feature is complete and you run with --production --datastore=s3 . During testing/development you can use --datastore=local (or if you specify METAFLOW_DEFAULT_DATASTORE=local in .envrc then you don't need to explicitly choose this) If the flow was run on batch, which requires the S3 datastore, (covered in a later episode) then the download and upload speeds would be faster and the slowdown not so severe If all else fails you can always use the local datastore and use metaflow.S3 to save and share the necessary data at the end of the flow Note: You should use the self context, i.e. with S3(run=self) as s3: ... so that data is versioned under the current run ID and doesn't risk overwriting someone else's version of the data.","title":"Companies House"},{"location":"guides/","text":"","title":"Index"},{"location":"guides/metaflow/","text":"Metaflow Guide \u00b6 Use-cases \u00b6 Share results (particularly long-running/complex) Version results Computing at scale Dependencies Best practices \u00b6 Writing flows \u00b6 Naming a flow No need to add Flow as a suffix, the directory structure should speak to that Don't make names too generic, e.g. if the flow trains a topic model on Arxiv paper abstracts then call it something like ArxivAbstractTopicModel rather than just TopicModel Add a test mode If your flow takes more than a minute or two to run then you should always add a parameter test_mode = Parameter(\"test-mode\", ...) (by convention) which will run your flow in a test mode that runs quickly, e.g. fetch/run a subset of the data. This facilitates both efficient code review (a reviewer can check the test mode runs successfully without having to wait hours/days for the full flow to run) and automated testing. We recommend that by default self.test_mode is either: The logical not of current.is_production ( default=lambda _: not current.is_production ) True If a flow is run with --production ( current.is_production is True ) then test mode functionality should not be activated! Whilst often the logical not of one another, self.test_mode and current.is_production are two different concepts - self.test_mode and current.is_production may both be False when a user wants to run a flow without affecting other users. The only class/function in a flow file should be the Flow itself Flows should use the @project decorator (see workflow ) using a consistent name parameter throughout the project Flow steps should be as minimal as possible containing: Setup such as parsing and logging parameters or declaring context managers At most a few high-level function calls Any flow-level logic - e.g. merging artifacts Add type-annotations for the data artifacts Imports used within a step should always go within a step (at the top of it) Because metaflow has the ability to abstract infrastructure then different steps may run on different machines with different environments. from kuebiko.pipeline.sic.utils import * wouldn't work in a step - kuebiko isn't installed/packaged in the new conda environment. Avoiding saving data artifacts as dataframes (or other library-specific classes) favouring standard Python data-structures instead. Python data-structures do not impose the Pandas dependency on the downstream consumer of the data who may be working in an environment where Pandas isn't available (e.g. AWS lambda) or may have a different version of Pandas which when your artifact is loaded may subtly differ or fail to load. If dataframes are persisted as regular Python data-structure, the downstream consumer can still generate a dataframe if they want. Only use Metaflow's conda decorators when you definitely need them Docstrings for steps: Always for start and end - their name cannot convey any information about what the step does For any step whose name cannot convey the essence of the step or for complex steps (steps that are more than one or two well documented function calls) Workflow \u00b6 Use local datastore during development Either pass --datastore=local when running a flow or add METAFLOW_DEFAULT_DATASTORE=local to .envrc to change the default datastore. This reduces the overhead to run a flow but means the results are local to your machine (the metadata of the run is still stored in an AWS RDS instance). Use datastore=s3 with --production to run your flow such that the artifacts can be seen by another user. Use the namespace relating to your @project name, e.g. if you're using @project(name=\"kuebiko\") for your flows, use metaflow.namespace(\"project:kuebiko\") Setting that in kuebiko/__init__.py means it gets set whenever you use a getter Things to watch out for \u00b6 Gotchas \u00b6 You cannot save functions as data artifacts, they get ignored. You can however save data artifacts with functions inside, e.g. self.artifacts = {\"key\": lambda _: \"value\"} Bugs \u00b6 Here are a bugs you may encounter: Using resume on a failed flow that uses metaflow.S3 may fail/be inconsistent https://github.com/Netflix/metaflow/issues/664 Metaflow's Conda decorators don't work with Python 3.9 https://github.com/Netflix/metaflow/issues/399 Poor errors \u00b6 Here are errors you may encounter which are uninformative: TypeError: Can't mix strings and bytes in path components - Use of metaflow.S3 with an incorrect metaflow profile","title":"Metaflow"},{"location":"guides/metaflow/#metaflow-guide","text":"","title":"Metaflow Guide"},{"location":"guides/metaflow/#use-cases","text":"Share results (particularly long-running/complex) Version results Computing at scale Dependencies","title":"Use-cases"},{"location":"guides/metaflow/#best-practices","text":"","title":"Best practices"},{"location":"guides/metaflow/#writing-flows","text":"Naming a flow No need to add Flow as a suffix, the directory structure should speak to that Don't make names too generic, e.g. if the flow trains a topic model on Arxiv paper abstracts then call it something like ArxivAbstractTopicModel rather than just TopicModel Add a test mode If your flow takes more than a minute or two to run then you should always add a parameter test_mode = Parameter(\"test-mode\", ...) (by convention) which will run your flow in a test mode that runs quickly, e.g. fetch/run a subset of the data. This facilitates both efficient code review (a reviewer can check the test mode runs successfully without having to wait hours/days for the full flow to run) and automated testing. We recommend that by default self.test_mode is either: The logical not of current.is_production ( default=lambda _: not current.is_production ) True If a flow is run with --production ( current.is_production is True ) then test mode functionality should not be activated! Whilst often the logical not of one another, self.test_mode and current.is_production are two different concepts - self.test_mode and current.is_production may both be False when a user wants to run a flow without affecting other users. The only class/function in a flow file should be the Flow itself Flows should use the @project decorator (see workflow ) using a consistent name parameter throughout the project Flow steps should be as minimal as possible containing: Setup such as parsing and logging parameters or declaring context managers At most a few high-level function calls Any flow-level logic - e.g. merging artifacts Add type-annotations for the data artifacts Imports used within a step should always go within a step (at the top of it) Because metaflow has the ability to abstract infrastructure then different steps may run on different machines with different environments. from kuebiko.pipeline.sic.utils import * wouldn't work in a step - kuebiko isn't installed/packaged in the new conda environment. Avoiding saving data artifacts as dataframes (or other library-specific classes) favouring standard Python data-structures instead. Python data-structures do not impose the Pandas dependency on the downstream consumer of the data who may be working in an environment where Pandas isn't available (e.g. AWS lambda) or may have a different version of Pandas which when your artifact is loaded may subtly differ or fail to load. If dataframes are persisted as regular Python data-structure, the downstream consumer can still generate a dataframe if they want. Only use Metaflow's conda decorators when you definitely need them Docstrings for steps: Always for start and end - their name cannot convey any information about what the step does For any step whose name cannot convey the essence of the step or for complex steps (steps that are more than one or two well documented function calls)","title":"Writing flows"},{"location":"guides/metaflow/#workflow","text":"Use local datastore during development Either pass --datastore=local when running a flow or add METAFLOW_DEFAULT_DATASTORE=local to .envrc to change the default datastore. This reduces the overhead to run a flow but means the results are local to your machine (the metadata of the run is still stored in an AWS RDS instance). Use datastore=s3 with --production to run your flow such that the artifacts can be seen by another user. Use the namespace relating to your @project name, e.g. if you're using @project(name=\"kuebiko\") for your flows, use metaflow.namespace(\"project:kuebiko\") Setting that in kuebiko/__init__.py means it gets set whenever you use a getter","title":"Workflow"},{"location":"guides/metaflow/#things-to-watch-out-for","text":"","title":"Things to watch out for"},{"location":"guides/metaflow/#gotchas","text":"You cannot save functions as data artifacts, they get ignored. You can however save data artifacts with functions inside, e.g. self.artifacts = {\"key\": lambda _: \"value\"}","title":"Gotchas"},{"location":"guides/metaflow/#bugs","text":"Here are a bugs you may encounter: Using resume on a failed flow that uses metaflow.S3 may fail/be inconsistent https://github.com/Netflix/metaflow/issues/664 Metaflow's Conda decorators don't work with Python 3.9 https://github.com/Netflix/metaflow/issues/399","title":"Bugs"},{"location":"guides/metaflow/#poor-errors","text":"Here are errors you may encounter which are uninformative: TypeError: Can't mix strings and bytes in path components - Use of metaflow.S3 with an incorrect metaflow profile","title":"Poor errors"}]}