{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"ds-cookiecutter guide (Kuebiko) \u00b6 Kuebiko (\u4e45\u5ef6\u6bd8\u53e4) is the Shinto kami (\"god; deity\") of Folk wisdom, knowledge and agriculture, and is represented in Japanese mythology as a scarecrow who cannot walk but has comprehensive awareness. An INCOMPLETE \"full\" (see Corners cut ) example ds-cookiecutter project broken down into episodes, with annotations/narrative for each episode focusing on a different aspect. In addition, there are a set of reference guides consolidating key concepts and pointing to further resources. Not from around these parts? A disclaimer... This is an internal (but open) project to develop and refine best practices for a subset of Nesta's work delivering data science projects. It is not a guide to production data science and the recommended practices in this guide may be wholly unsuitable for you/your company because you may be delivering completely different types of projects under completely different constraints. Furthermore, these docs are deployed when a new episode is ready for review therefore content is likely to change significantly ! To get an indication of what content is stable, check out which episodes have closed issues . Project problem statement \u00b6 You have been given a big list (>100,000) of URLs corresponding to the business websites of UK companies. The stakeholder is interested in better understanding the performance, accessibility, and legal compliance of business websites across different industries (SIC codes). Some specific questions they have are: It is a requirement that limited companies must display the company's registered number on their website, how many businesses actually list a company number? Does the download size, response time, or reachability of a business' website bear any relation to the number of web development businesses nearby? How do web development businesses themselves do on the extracted features mentioned above? Is there any relationship between aforementioned extracted features and text on a business' website relating to what a business does? Episode plan \u00b6 Below is a list of provisional episodes. See issues labeled episode for more information and more timely updates. [x] 0. Setup [x] 1. Metaflow basics - fetching auxiliary data from the web [ ] ~2. Getting data from SQL - fetching NOMIS auxiliary data~ [ ] 3. Critical coding - web-scraping utilities [ ] 4. Metaflow on AWS batch - scaling a web-scraper [ ] 5. Advanced data getters - extracting and explored scraped data [ ] ~6. Data analysis - answering stakeholders first 3 questions~ [ ] ~7. Perfect is the enemy of good - Company description extraction~ [ ] ~8. Metaflow on AWS batch with a GPU - transformers~ [ ] ~9. The fine line between analysis and pipeline - answering stakeholders final question~ [ ] ~10. Refactoring~ Further episodes may be added as further use-cases arise, further guidelines are developed or new technologies adopted (e.g. AWS step-functions). How to read/use \u00b6 For each released episode the episode overview gives: a summary of what is tackled in the episode (both features but also associated guides); a description of the important new/modified files; and any important notes such as whether future episodes will improve upon specific aspects and whether an episode stands alone or builds upon a previous episode. For each episode there is a page at episodes/episode_XX which is the starting point for the episode. It will walk through the episode referring to files. Files with a * at the end are themselves annotated and you should read through these files and their annotations as the text refers to them. Normal python comments, e.g. # hello there , belong in a real (not an example) data science project; however comments with two hashes like ## hello there are for the narrative purposes of this guide. In the online documentation, narrative comments are rendered as markdown on the left hand side of the page and ordinary comments are kept within the rendered source. Note: If reading the raw source files or using the github UI then some links may not work as this repo is targeting the online docs first. Where links don't work, the link text should be informative enough to send you to the right place! Corners cut \u00b6 This is a MVP of a data science project following the ds-cookiecutter workflow but some corners have been cut: The problem statement is a bit contrived. It was chosen in order to demonstrate a variety of day-to-day aspects of data science development whilst keeping the overall level of content as small as possible to aid accessibility. We present and discuss complete, but not necessarily final, features - e.g. models work end-to-end but may only be simple. A detailed iterative workflow such as exploring in notebooks and then refactoring into modules, is not covered because: This would further bloat the already ambitious scope of this repository Everyone works differently and projects are nuanced therefore it does not pay to be opinonated here There are plans to record a video demoing and discussing one possible workflow Development/commit order is chosen for episodic release not necessarily the order which a real project would have Miscellaneous notes \u00b6 To avoid covering too much at once, early episodes may neglect important things such as tests and data quality and data validation. In a real project tests should always be written at the same time as the code they test (or even before if you are a proponent of test driven development)! When an episode gets opened for review then the code is only in a state/quality that any other PR might be at when a review is requested. I.e. there will be mistakes/shortcomings so it's up to DAP as a community to help improve things. There are a few deliberate \"mistakes\" throughout, try and spot them and raise it in review. Based on ds-cookiecutter==0.1.1","title":"Home"},{"location":"#ds-cookiecutter-guide-kuebiko","text":"Kuebiko (\u4e45\u5ef6\u6bd8\u53e4) is the Shinto kami (\"god; deity\") of Folk wisdom, knowledge and agriculture, and is represented in Japanese mythology as a scarecrow who cannot walk but has comprehensive awareness. An INCOMPLETE \"full\" (see Corners cut ) example ds-cookiecutter project broken down into episodes, with annotations/narrative for each episode focusing on a different aspect. In addition, there are a set of reference guides consolidating key concepts and pointing to further resources. Not from around these parts? A disclaimer... This is an internal (but open) project to develop and refine best practices for a subset of Nesta's work delivering data science projects. It is not a guide to production data science and the recommended practices in this guide may be wholly unsuitable for you/your company because you may be delivering completely different types of projects under completely different constraints. Furthermore, these docs are deployed when a new episode is ready for review therefore content is likely to change significantly ! To get an indication of what content is stable, check out which episodes have closed issues .","title":"ds-cookiecutter guide (Kuebiko)"},{"location":"#project-problem-statement","text":"You have been given a big list (>100,000) of URLs corresponding to the business websites of UK companies. The stakeholder is interested in better understanding the performance, accessibility, and legal compliance of business websites across different industries (SIC codes). Some specific questions they have are: It is a requirement that limited companies must display the company's registered number on their website, how many businesses actually list a company number? Does the download size, response time, or reachability of a business' website bear any relation to the number of web development businesses nearby? How do web development businesses themselves do on the extracted features mentioned above? Is there any relationship between aforementioned extracted features and text on a business' website relating to what a business does?","title":"Project problem statement"},{"location":"#episode-plan","text":"Below is a list of provisional episodes. See issues labeled episode for more information and more timely updates. [x] 0. Setup [x] 1. Metaflow basics - fetching auxiliary data from the web [ ] ~2. Getting data from SQL - fetching NOMIS auxiliary data~ [ ] 3. Critical coding - web-scraping utilities [ ] 4. Metaflow on AWS batch - scaling a web-scraper [ ] 5. Advanced data getters - extracting and explored scraped data [ ] ~6. Data analysis - answering stakeholders first 3 questions~ [ ] ~7. Perfect is the enemy of good - Company description extraction~ [ ] ~8. Metaflow on AWS batch with a GPU - transformers~ [ ] ~9. The fine line between analysis and pipeline - answering stakeholders final question~ [ ] ~10. Refactoring~ Further episodes may be added as further use-cases arise, further guidelines are developed or new technologies adopted (e.g. AWS step-functions).","title":"Episode plan"},{"location":"#how-to-readuse","text":"For each released episode the episode overview gives: a summary of what is tackled in the episode (both features but also associated guides); a description of the important new/modified files; and any important notes such as whether future episodes will improve upon specific aspects and whether an episode stands alone or builds upon a previous episode. For each episode there is a page at episodes/episode_XX which is the starting point for the episode. It will walk through the episode referring to files. Files with a * at the end are themselves annotated and you should read through these files and their annotations as the text refers to them. Normal python comments, e.g. # hello there , belong in a real (not an example) data science project; however comments with two hashes like ## hello there are for the narrative purposes of this guide. In the online documentation, narrative comments are rendered as markdown on the left hand side of the page and ordinary comments are kept within the rendered source. Note: If reading the raw source files or using the github UI then some links may not work as this repo is targeting the online docs first. Where links don't work, the link text should be informative enough to send you to the right place!","title":"How to read/use"},{"location":"#corners-cut","text":"This is a MVP of a data science project following the ds-cookiecutter workflow but some corners have been cut: The problem statement is a bit contrived. It was chosen in order to demonstrate a variety of day-to-day aspects of data science development whilst keeping the overall level of content as small as possible to aid accessibility. We present and discuss complete, but not necessarily final, features - e.g. models work end-to-end but may only be simple. A detailed iterative workflow such as exploring in notebooks and then refactoring into modules, is not covered because: This would further bloat the already ambitious scope of this repository Everyone works differently and projects are nuanced therefore it does not pay to be opinonated here There are plans to record a video demoing and discussing one possible workflow Development/commit order is chosen for episodic release not necessarily the order which a real project would have","title":"Corners cut"},{"location":"#miscellaneous-notes","text":"To avoid covering too much at once, early episodes may neglect important things such as tests and data quality and data validation. In a real project tests should always be written at the same time as the code they test (or even before if you are a proponent of test driven development)! When an episode gets opened for review then the code is only in a state/quality that any other PR might be at when a review is requested. I.e. there will be mistakes/shortcomings so it's up to DAP as a community to help improve things. There are a few deliberate \"mistakes\" throughout, try and spot them and raise it in review. Based on ds-cookiecutter==0.1.1","title":"Miscellaneous notes"},{"location":"glossary/","text":"Glossary \u00b6 DAG - Directed Acyclic Graph \u00b6 Critical coding \u00b6 SRP - Single Responsibilty Principle \u00b6 A module, class, or function should only have a single responsibility. Put alternatively, something obeying the SRP should only have one reason to change. DRY - Don't repeat yourself \u00b6 If you find yourself writing code to do the same thing many times then it's a very strong hint that you should abstract the repeated work.","title":"Glossary"},{"location":"glossary/#glossary","text":"","title":"Glossary"},{"location":"glossary/#dag-directed-acyclic-graph","text":"","title":"DAG - Directed Acyclic Graph"},{"location":"glossary/#critical-coding","text":"","title":"Critical coding"},{"location":"glossary/#srp-single-responsibilty-principle","text":"A module, class, or function should only have a single responsibility. Put alternatively, something obeying the SRP should only have one reason to change.","title":"SRP - Single Responsibilty Principle"},{"location":"glossary/#dry-dont-repeat-yourself","text":"If you find yourself writing code to do the same thing many times then it's a very strong hint that you should abstract the repeated work.","title":"DRY - Don't repeat yourself"},{"location":"episodes/","text":"Episode overview \u00b6 Overview of released episodes content, see issues labeled episode for planned episodes. 1. Metaflow basics - fetching auxillary data from the web \u00b6 This episode walks through the basics of using Metaflow idiomatically - i.e. in a way that suits DAP and its use-cases - by fetching three auxilliary datasets needed: Companies House lookups to get the SIC code and address for each company number. National Statistics Postcode Lookup (NSPL) which will allow us to identify the Local Authority District (LADs) a company belongs to by matching its postcode. SIC taxonomy lookup between names and codes In addition, the first set of content (later episodes add more advanced content) is added to a Metaflow guide. Important notes \u00b6 There are missing pieces to this episode that a data-science PR should have. The most obvious of these is the absence of tests which are the subject of episode 3. Tests for the three pipelines of this episode will be added in the testing episode in order to keep the content of this episode focused around writing basic Metaflow flows and getters. Besides the episode guide there is neither documentation of how to run the flows or version-controlled configuration for flow parameters. This will be addressed in episode 4. Key files \u00b6 Episode 1 guide kuebiko/pipeline/sic/flow.py kuebiko/getters/sic.py kuebiko/pipeline/nspl/flow.py kuebiko/getters/nspl.py kuebiko/pipeline/companies_house/flow.py kuebiko/getters/companies_house.py Metaflow guide","title":"Episode overview"},{"location":"episodes/#episode-overview","text":"Overview of released episodes content, see issues labeled episode for planned episodes.","title":"Episode overview"},{"location":"episodes/#1-metaflow-basics-fetching-auxillary-data-from-the-web","text":"This episode walks through the basics of using Metaflow idiomatically - i.e. in a way that suits DAP and its use-cases - by fetching three auxilliary datasets needed: Companies House lookups to get the SIC code and address for each company number. National Statistics Postcode Lookup (NSPL) which will allow us to identify the Local Authority District (LADs) a company belongs to by matching its postcode. SIC taxonomy lookup between names and codes In addition, the first set of content (later episodes add more advanced content) is added to a Metaflow guide.","title":"1. Metaflow basics - fetching auxillary data from the web"},{"location":"episodes/#important-notes","text":"There are missing pieces to this episode that a data-science PR should have. The most obvious of these is the absence of tests which are the subject of episode 3. Tests for the three pipelines of this episode will be added in the testing episode in order to keep the content of this episode focused around writing basic Metaflow flows and getters. Besides the episode guide there is neither documentation of how to run the flows or version-controlled configuration for flow parameters. This will be addressed in episode 4.","title":"Important notes"},{"location":"episodes/#key-files","text":"Episode 1 guide kuebiko/pipeline/sic/flow.py kuebiko/getters/sic.py kuebiko/pipeline/nspl/flow.py kuebiko/getters/nspl.py kuebiko/pipeline/companies_house/flow.py kuebiko/getters/companies_house.py Metaflow guide","title":"Key files"},{"location":"episodes/episode_01/","text":"1. Metaflow basics - fetching auxillary data from the web \u00b6 This is not a Metaflow tutorial! It is assumed that you have already looked through the official Metaflow tutorials and familiarised yourself with the Metaflow docs . Standard Industrial Taxonomy (SIC) \u00b6 The first dataset that is fetched and processed is an Excel spreadsheet containing the codes and names of each of the 5 levels of the Standard Industrial Classification (SIC) taxonomy. Later in the project when we come to analyse whether industry has an effect on the performance of a business' website this will be done using the SIC taxonomy - it's important that we are able to convert the (zero-padded) numeric codes into their description names when interpreting and reporting results. The flow code can found in kuebiko/pipeline/sic/flow.py * and its utility functions (which are not a focus of this episode) in kuebiko/pipeline/sic/utils.py . Lets recap a few Metaflow commands: python kuebiko/pipeline/sic/flow.py show will give us the docstrings and basic structure of the flow. python kuebiko/pipeline/sic/flow.py run will run the flow. python kuebiko/pipeline/sic/flow.py dump <run id>/end will dump a summary of the artifacts for a given run ID. Metaflow defaults .envrc specifies: METAFLOW_DEFAULT_DATASTORE=local METAFLOW_PROFILE=ds-cookiecutter direnv automatically loads these environment variables (after you run direnv allow - a security mechanism to avoid changes to .envrc automatically executing unsafe code). The end effect is that Metaflow will use the configuration in ~/.metaflowconfig/config_ds-cookiecutter.json that ds-cookiecutter sets up, and then override the default datastore to be local (data artifacts are persisted on your local machine in a .metaflow/ folder not to S3). python kuebiko/pipeline/sic/flow.py --datastore=s3 run will run the flow storing artifacts in S3 (rather than locally). python kuebiko/pipeline/sic/flow.py --datastore=s3 --production run will run the flow storing artifacts in S3 (rather than locally) under a production namespace. kuebiko/getters/sic.py * implements a getter function to load the lookups from our flow. Writing a getter may seem to be an uneccessary level of indirection; however it is important to have this level of indirection because getters : Allows us to create derived views of data where Metaflow artifacts may not map 1:1 to getter functions Provides a place to go to discover what data is available for analysis along in a logical folder/file structure with clear doc-strings per function It is very important that getters relate to the problem domain , get_csv_table(filename: str) -> DataFrame is not a valid getter, it's a utility function at best! Enables the possibility of extra data validation before returning to the user National Statistics Postcode Lookup (NSPL) \u00b6 The second dataset that is fetched and processed is the National Statistics Postcode Lookup (NSPL), a lookup from UK postcodes to various statistical geographies. In this project we wish to extract the Local Authority District (variably referred to as LAD or LAUA), latitude, and longitude of each postcode. The Local Authority Districts will enable spatial aggregation of businesses to a level at which official statistics relating to the number of businesses in each SIC code in a given area. The latitude and longitude will enable other activities such as plotting. The flow code can found in kuebiko/pipeline/nspl/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/nspl/utils.py , and its getters in kuebiko/getters/nspl.py * Info If a flow uses one of Metaflow's Conda decorators, or --with conda at runtime then it must run with --environment=conda . Running python kuebiko/pipeline/nspl/flow.py --environment=conda run will run the flow. Check out the data quality report in the Metaflow UI Companies House \u00b6 The third dataset that is fetched and processed is the Companies House (UK company registrar) monthly data dump. Each registered UK company has a company number. By finding company numbers on websites we can match companies to Companies House obtaining information such as the SIC code they classify their activities as and their registered trading addresses. The flow code can found in kuebiko/pipeline/companies_house/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/companies_house/utils.py , and its getters in kuebiko/getters/companies_house.py * . Running python kuebiko/pipeline/companies_house/flow.py --environment conda run will run the flow. Check out the data quality reports in the Metaflow UI A short investigation into artifact overheads \u00b6 In kuebiko/pipeline/sic/flow.py * we talked about the size of metaflow steps and the fact that persisting artifacts with the S3 datastore has some overhead, but how much? The Companies House dataset is quite large (several GB of RAM as a Pandas dataframe), so lets use this to explore the slowdown we might see close to the worst case (also comparing to plain Python kuebiko/pipeline/companies_house/script.py * whilst we're here): time python kuebiko/pipeline/companies_house/flow.py run --max-workers 1 ~10 minutes time python kuebiko/pipeline/companies_house/script.py ~10 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 1 ~26 minutes time python kuebiko/pipeline/companies_house/flow.py run --max-workers 4 ~4 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 4 ~19 minutes Firstly, we see that Metaflow is the same speed as the plain Python script when running on one core. We might expect Metaflow to be a little slower because it has to serialise and deserialise artifacts between steps. Metaflow is twice as fast when we allow up to 4 steps to execute at once - free parallelism as long as we have the RAM to spare! Next, comparing the local datastore to the S3 datastore, we see a rather large slowdown of 2.5x (5x with more parallelism)! Do not despair: This was picked as a particularly bad case In reality, you only need to pay this cost once when your feature is complete and you run with --production --datastore=s3 . During testing/development you can use --datastore=local (or if you specify METAFLOW_DEFAULT_DATASTORE=local in .envrc then you don't need to explicitly choose this) If the flow was run on batch, which requires the S3 datastore, (covered in a later episode) then the download and upload speeds would be faster and the slowdown not so severe If all else fails you can always use the local datastore and use metaflow.S3 to save and share the necessary data at the end of the flow Note: You should use the self context, i.e. with S3(run=self) as s3: ... so that data is versioned under the current run ID and doesn't risk overwriting someone else's version of the data.","title":"01 - Metaflow basics"},{"location":"episodes/episode_01/#1-metaflow-basics-fetching-auxillary-data-from-the-web","text":"This is not a Metaflow tutorial! It is assumed that you have already looked through the official Metaflow tutorials and familiarised yourself with the Metaflow docs .","title":"1. Metaflow basics - fetching auxillary data from the web"},{"location":"episodes/episode_01/#standard-industrial-taxonomy-sic","text":"The first dataset that is fetched and processed is an Excel spreadsheet containing the codes and names of each of the 5 levels of the Standard Industrial Classification (SIC) taxonomy. Later in the project when we come to analyse whether industry has an effect on the performance of a business' website this will be done using the SIC taxonomy - it's important that we are able to convert the (zero-padded) numeric codes into their description names when interpreting and reporting results. The flow code can found in kuebiko/pipeline/sic/flow.py * and its utility functions (which are not a focus of this episode) in kuebiko/pipeline/sic/utils.py . Lets recap a few Metaflow commands: python kuebiko/pipeline/sic/flow.py show will give us the docstrings and basic structure of the flow. python kuebiko/pipeline/sic/flow.py run will run the flow. python kuebiko/pipeline/sic/flow.py dump <run id>/end will dump a summary of the artifacts for a given run ID. Metaflow defaults .envrc specifies: METAFLOW_DEFAULT_DATASTORE=local METAFLOW_PROFILE=ds-cookiecutter direnv automatically loads these environment variables (after you run direnv allow - a security mechanism to avoid changes to .envrc automatically executing unsafe code). The end effect is that Metaflow will use the configuration in ~/.metaflowconfig/config_ds-cookiecutter.json that ds-cookiecutter sets up, and then override the default datastore to be local (data artifacts are persisted on your local machine in a .metaflow/ folder not to S3). python kuebiko/pipeline/sic/flow.py --datastore=s3 run will run the flow storing artifacts in S3 (rather than locally). python kuebiko/pipeline/sic/flow.py --datastore=s3 --production run will run the flow storing artifacts in S3 (rather than locally) under a production namespace. kuebiko/getters/sic.py * implements a getter function to load the lookups from our flow. Writing a getter may seem to be an uneccessary level of indirection; however it is important to have this level of indirection because getters : Allows us to create derived views of data where Metaflow artifacts may not map 1:1 to getter functions Provides a place to go to discover what data is available for analysis along in a logical folder/file structure with clear doc-strings per function It is very important that getters relate to the problem domain , get_csv_table(filename: str) -> DataFrame is not a valid getter, it's a utility function at best! Enables the possibility of extra data validation before returning to the user","title":"Standard Industrial Taxonomy (SIC)"},{"location":"episodes/episode_01/#national-statistics-postcode-lookup-nspl","text":"The second dataset that is fetched and processed is the National Statistics Postcode Lookup (NSPL), a lookup from UK postcodes to various statistical geographies. In this project we wish to extract the Local Authority District (variably referred to as LAD or LAUA), latitude, and longitude of each postcode. The Local Authority Districts will enable spatial aggregation of businesses to a level at which official statistics relating to the number of businesses in each SIC code in a given area. The latitude and longitude will enable other activities such as plotting. The flow code can found in kuebiko/pipeline/nspl/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/nspl/utils.py , and its getters in kuebiko/getters/nspl.py * Info If a flow uses one of Metaflow's Conda decorators, or --with conda at runtime then it must run with --environment=conda . Running python kuebiko/pipeline/nspl/flow.py --environment=conda run will run the flow. Check out the data quality report in the Metaflow UI","title":"National Statistics Postcode Lookup (NSPL)"},{"location":"episodes/episode_01/#companies-house","text":"The third dataset that is fetched and processed is the Companies House (UK company registrar) monthly data dump. Each registered UK company has a company number. By finding company numbers on websites we can match companies to Companies House obtaining information such as the SIC code they classify their activities as and their registered trading addresses. The flow code can found in kuebiko/pipeline/companies_house/flow.py * , its utility functions (which are not a focus of this episode) in kuebiko/pipeline/companies_house/utils.py , and its getters in kuebiko/getters/companies_house.py * . Running python kuebiko/pipeline/companies_house/flow.py --environment conda run will run the flow. Check out the data quality reports in the Metaflow UI","title":"Companies House"},{"location":"episodes/episode_01/#a-short-investigation-into-artifact-overheads","text":"In kuebiko/pipeline/sic/flow.py * we talked about the size of metaflow steps and the fact that persisting artifacts with the S3 datastore has some overhead, but how much? The Companies House dataset is quite large (several GB of RAM as a Pandas dataframe), so lets use this to explore the slowdown we might see close to the worst case (also comparing to plain Python kuebiko/pipeline/companies_house/script.py * whilst we're here): time python kuebiko/pipeline/companies_house/flow.py run --max-workers 1 ~10 minutes time python kuebiko/pipeline/companies_house/script.py ~10 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 1 ~26 minutes time python kuebiko/pipeline/companies_house/flow.py run --max-workers 4 ~4 minutes time python kuebiko/pipeline/companies_house/flow.py --datastore=s3 run --max-workers 4 ~19 minutes Firstly, we see that Metaflow is the same speed as the plain Python script when running on one core. We might expect Metaflow to be a little slower because it has to serialise and deserialise artifacts between steps. Metaflow is twice as fast when we allow up to 4 steps to execute at once - free parallelism as long as we have the RAM to spare! Next, comparing the local datastore to the S3 datastore, we see a rather large slowdown of 2.5x (5x with more parallelism)! Do not despair: This was picked as a particularly bad case In reality, you only need to pay this cost once when your feature is complete and you run with --production --datastore=s3 . During testing/development you can use --datastore=local (or if you specify METAFLOW_DEFAULT_DATASTORE=local in .envrc then you don't need to explicitly choose this) If the flow was run on batch, which requires the S3 datastore, (covered in a later episode) then the download and upload speeds would be faster and the slowdown not so severe If all else fails you can always use the local datastore and use metaflow.S3 to save and share the necessary data at the end of the flow Note: You should use the self context, i.e. with S3(run=self) as s3: ... so that data is versioned under the current run ID and doesn't risk overwriting someone else's version of the data.","title":"A short investigation into artifact overheads"},{"location":"episodes/episode_03/","text":"3. Critical coding - web-scraping utilities \u00b6 This episode is highly incomplete and does not do a good job of demonstrating critical coding... There are missing tests; the code is still quite messy (e.g. the exposed API could be improved and exceptions are relied on for some control flow); several concepts are glossed over; and the ## annotations are not coherent. On reflection, the complexity of interacting with a web-browser (and everything that can go wrong in doing so) and the level of prior knowledge required means that this was probably a poor choice of task to pick for discussing critical coding. A lot of the modern web relies on Javascript to do just about everything, if we just use requests then many web-pages will not render content => use Selenium Selenium is meant to be used to test applications (implying prior knowledge of the website structure - e.g. the page is loaded when we can find a <div> with a specific class/ID - but we are using it to scrape thousands of websites we have no prior knowledge of which adds significant complexity, e.g.: We don't know if the website is reachable The website might crash the browser We don't know when a webpage can be considered loaded. There could be requests for content that take ages or timeout - these could be requests for crucial content or a tiny script snippet to load an ad. Because of these sources of complexity and the fact that we want to build a scraping pipeline that is robust to failure (we don't want one unreachable website or a (recoverable) selenium driver crash to cause the whole pipeline to fail) we've elected to create a small utility library to help make the task of using Selenium for web-scraping a little easier. kuebiko/utils/url.py kuebiko/utils/selenium/ constants.py - Defines constants, e.g. default timeouts and chrome network error codes exceptions.py - Defines two new exception types we need to handle errors driver.py - Provides a wrapper for Selenium WebDriver s that enables recovery from crashes error_handling.py - Defines error handling functions that map from the errors that Chrome/Selenium give us, to a smaller set of errors closer to the domain of scraping utils.py - General utility functions for user-code use (bad catch-all naming) get_page.py - User functions ... kuebiko/utils/{tests/,selenium/tests} - Tests","title":"03 - [Incomplete]"},{"location":"episodes/episode_03/#3-critical-coding-web-scraping-utilities","text":"This episode is highly incomplete and does not do a good job of demonstrating critical coding... There are missing tests; the code is still quite messy (e.g. the exposed API could be improved and exceptions are relied on for some control flow); several concepts are glossed over; and the ## annotations are not coherent. On reflection, the complexity of interacting with a web-browser (and everything that can go wrong in doing so) and the level of prior knowledge required means that this was probably a poor choice of task to pick for discussing critical coding. A lot of the modern web relies on Javascript to do just about everything, if we just use requests then many web-pages will not render content => use Selenium Selenium is meant to be used to test applications (implying prior knowledge of the website structure - e.g. the page is loaded when we can find a <div> with a specific class/ID - but we are using it to scrape thousands of websites we have no prior knowledge of which adds significant complexity, e.g.: We don't know if the website is reachable The website might crash the browser We don't know when a webpage can be considered loaded. There could be requests for content that take ages or timeout - these could be requests for crucial content or a tiny script snippet to load an ad. Because of these sources of complexity and the fact that we want to build a scraping pipeline that is robust to failure (we don't want one unreachable website or a (recoverable) selenium driver crash to cause the whole pipeline to fail) we've elected to create a small utility library to help make the task of using Selenium for web-scraping a little easier. kuebiko/utils/url.py kuebiko/utils/selenium/ constants.py - Defines constants, e.g. default timeouts and chrome network error codes exceptions.py - Defines two new exception types we need to handle errors driver.py - Provides a wrapper for Selenium WebDriver s that enables recovery from crashes error_handling.py - Defines error handling functions that map from the errors that Chrome/Selenium give us, to a smaller set of errors closer to the domain of scraping utils.py - General utility functions for user-code use (bad catch-all naming) get_page.py - User functions ... kuebiko/utils/{tests/,selenium/tests} - Tests","title":"3. Critical coding - web-scraping utilities"},{"location":"episodes/episode_04/","text":"4. Metaflow on AWS batch \u00b6 This episode is partially complete Takes the scraping utilities developed in Episode 3 and wraps them in a Metaflow flow that starts many concurrent Batch jobs running our scraper allowing it to scale to hundreds of thousands of websites. It also introduces the metaflow_extensions library which provides a @pip decorator which allows easy installation of pip dependencies. This is useful because it allows us to use the scraping utilities (in kuebiko/utils/* ) on AWS Batch and/or within a Conda environment. kuebiko/pipeline/scraper/flow.py defines the flow kuebiko/pipeline/scraper/utils.py contains related utility functions: chrome_scraper - Define a Selenium Chrome WebDriver instance specific to scraping and performance benchmarking websites - this gets injected into the scraper utilities get_page - Simple wrapper around the scraper utilities to: Perform callback actions we want to occur when the scraper GETs a page Extract and return network and page source data page_stats - Processes network data into appropriate summary statistics We can run the flow with the command... python kuebiko/pipeline/scraper/flow.py \\ --datastore s3 \\ --metadata service \\ --environment = conda \\ --package-suffixes .py,.md,.txt \\ run \\ --url-list inputs/data/websites.csv \\ --with \"batch:image=pyselenium\" where: --with: \"batch:image=pyselenium\" will run our flow steps on batch with the pyselenium image, which is a Docker image that will work with Python, Selenium, and Chrome and is hosted in Nesta's AWS ECR account alternatively to building, deploying, and maintaining a Docker image you could check out metaflow_extensions ' preinstall environment the purpose of this episode is not to teach anything to do with Docker but the Dockerfile and a simple script to deploy to ECR can be found at kuebiko/pipeline/scraper/{Dockerfile,build_deploy_docker.sh} --package-suffixes ... determines what file extensions get packaged up into batch jobs .py for our code .md because setup.py requires README.md .txt to include any requirements*.txt files If we were doing a production run for all 100K sites we would do something like... python kuebiko/pipeline/scraper/flow.py \\ --datastore s3 \\ --metadata service \\ --environment = conda \\ --package-suffixes .py,.md,.txt \\ --production \\ run \\ --url-list inputs/data/websites.csv \\ --with \"batch:image=pyselenium\" \\ --max-workers 34 \\ --max-num-splits 150 where: --max-workers 34 means we can have 34 metaflow tasks running at once - for this flow it means we'll have up to 34 AWS batch jobs concurrently scraping websites --max-num-splits 150 increases the number of \"splits\" a foreach step can contain. The default is 100 (to protect against accidentally creating a large number of jobs) but we end up with 101 so we increase this. Check out the results in the Metaflow UI","title":"04 - [Partially complete]"},{"location":"episodes/episode_04/#4-metaflow-on-aws-batch","text":"This episode is partially complete Takes the scraping utilities developed in Episode 3 and wraps them in a Metaflow flow that starts many concurrent Batch jobs running our scraper allowing it to scale to hundreds of thousands of websites. It also introduces the metaflow_extensions library which provides a @pip decorator which allows easy installation of pip dependencies. This is useful because it allows us to use the scraping utilities (in kuebiko/utils/* ) on AWS Batch and/or within a Conda environment. kuebiko/pipeline/scraper/flow.py defines the flow kuebiko/pipeline/scraper/utils.py contains related utility functions: chrome_scraper - Define a Selenium Chrome WebDriver instance specific to scraping and performance benchmarking websites - this gets injected into the scraper utilities get_page - Simple wrapper around the scraper utilities to: Perform callback actions we want to occur when the scraper GETs a page Extract and return network and page source data page_stats - Processes network data into appropriate summary statistics We can run the flow with the command... python kuebiko/pipeline/scraper/flow.py \\ --datastore s3 \\ --metadata service \\ --environment = conda \\ --package-suffixes .py,.md,.txt \\ run \\ --url-list inputs/data/websites.csv \\ --with \"batch:image=pyselenium\" where: --with: \"batch:image=pyselenium\" will run our flow steps on batch with the pyselenium image, which is a Docker image that will work with Python, Selenium, and Chrome and is hosted in Nesta's AWS ECR account alternatively to building, deploying, and maintaining a Docker image you could check out metaflow_extensions ' preinstall environment the purpose of this episode is not to teach anything to do with Docker but the Dockerfile and a simple script to deploy to ECR can be found at kuebiko/pipeline/scraper/{Dockerfile,build_deploy_docker.sh} --package-suffixes ... determines what file extensions get packaged up into batch jobs .py for our code .md because setup.py requires README.md .txt to include any requirements*.txt files If we were doing a production run for all 100K sites we would do something like... python kuebiko/pipeline/scraper/flow.py \\ --datastore s3 \\ --metadata service \\ --environment = conda \\ --package-suffixes .py,.md,.txt \\ --production \\ run \\ --url-list inputs/data/websites.csv \\ --with \"batch:image=pyselenium\" \\ --max-workers 34 \\ --max-num-splits 150 where: --max-workers 34 means we can have 34 metaflow tasks running at once - for this flow it means we'll have up to 34 AWS batch jobs concurrently scraping websites --max-num-splits 150 increases the number of \"splits\" a foreach step can contain. The default is 100 (to protect against accidentally creating a large number of jobs) but we end up with 101 so we increase this. Check out the results in the Metaflow UI","title":"4. Metaflow on AWS batch"},{"location":"episodes/episode_05/","text":"5. Advanced data getters \u00b6 This episode is partially complete Episode 4 scales a web scraper to hundreds of thousands of websites but the scraped data is not available as a Metaflow data artifact (i.e. ...) because it is too large for memory. This relatively short episode shows how we can write a data getter to pull all this data together relatively efficiently - without running out of RAM and caching data to disk (so that each call to the getter only has to make a trip to disk rather than downloading a lot of data over the network from S3). kuebiko/getters/websites.py defines two getters: get_page_source - Returns an iterable (not all loaded into RAM - unless you call something like list(my_iterable) ) of tuples containing the scraped URL and it's page source (HTML). get_page_network_data - Returns a list of tuples containing the scraped URL and it's network data. Both of these getters rely on the _get_page_data function which: Fetches the keys data artifact from the UkBusinessHomepageScrape run which corresponds to the S3 objects the scraped data chunks are stored at Returns the data from those keys one at a time using yield from which means that: only one key of data is fetched and loaded into RAM at once each \"row\" in the data of a key is yielded one-by-one - the from bit of yield from Uses the diskcache library to cache chunks of data to disk","title":"05 - [Partially complete]"},{"location":"episodes/episode_05/#5-advanced-data-getters","text":"This episode is partially complete Episode 4 scales a web scraper to hundreds of thousands of websites but the scraped data is not available as a Metaflow data artifact (i.e. ...) because it is too large for memory. This relatively short episode shows how we can write a data getter to pull all this data together relatively efficiently - without running out of RAM and caching data to disk (so that each call to the getter only has to make a trip to disk rather than downloading a lot of data over the network from S3). kuebiko/getters/websites.py defines two getters: get_page_source - Returns an iterable (not all loaded into RAM - unless you call something like list(my_iterable) ) of tuples containing the scraped URL and it's page source (HTML). get_page_network_data - Returns a list of tuples containing the scraped URL and it's network data. Both of these getters rely on the _get_page_data function which: Fetches the keys data artifact from the UkBusinessHomepageScrape run which corresponds to the S3 objects the scraped data chunks are stored at Returns the data from those keys one at a time using yield from which means that: only one key of data is fetched and loaded into RAM at once each \"row\" in the data of a key is yielded one-by-one - the from bit of yield from Uses the diskcache library to cache chunks of data to disk","title":"5. Advanced data getters"},{"location":"guides/","text":"","title":"Index"},{"location":"guides/metaflow/","text":"Metaflow Guide \u00b6 Recap: What can Metaflow do for you? \u00b6 Data versioning \u00b6 Each execution of a Flow has a unique run ID which data artifacts are stored under. The artifacts and metadata of each run can be fetched using the Metaflow client API . Scalable computing \u00b6 Metaflow abstracts infrastructure making horizontally and vertically scalable cloud computing easily available to Data Scientists. In some (not all) cases it's as easy as adding --with batch to the end of the command to run a flow! Sharing results \u00b6 Data artifacts of runs can be stored and versioned not only on your machine but also to AWS S3, meaning you can share your results easily. Get you out of dependency hell \u00b6 Ever run into a situation where one part of a project needs a package that conflicts with another? Metaflow allows you to specify dependencies at the flow level and automatically builds a Conda environment and runs your flow in it. Make your pipelines easier to understand \u00b6 Expressing a script as a DAG instead makes things a lot easier to understand. Metaflow also has commands such as show and output-dot which visualise this structure. Checkpointing \u00b6 It's annoying when your code has gotten halfway through a pipeline only to tail. Metaflow automatically checkpoints the steps of your flow and lets you resume a run after fixing any issues, saving you having to re-run the whole pipeline or maintain your own checkpointing logic (that may be fraught with conflicts between different run parameters). Suggested practices \u00b6 Writing flows \u00b6 Naming a flow No need to add Flow as a suffix, the directory structure should speak to that Don't make names too generic, e.g. if the flow trains a topic model on Arxiv paper abstracts then call it something like ArxivAbstractTopicModel rather than just TopicModel Add a test mode If your flow takes more than a minute or two to run then you should always add a parameter test_mode = Parameter(\"test-mode\", ...) (by convention) which will run your flow in a test mode that runs quickly, e.g. fetch/run a subset of the data. This facilitates both efficient code review (a reviewer can check the test mode runs successfully without having to wait hours/days for the full flow to run) and automated testing. We recommend that by default self.test_mode is either: The logical not of current.is_production ( default=lambda _: not current.is_production ) True If a flow is run with --production ( current.is_production is True ) then test mode functionality should not be activated! Whilst often the logical not of one another, self.test_mode and current.is_production are two different concepts - self.test_mode and current.is_production may both be False when a user wants to run a flow without affecting other users. The only class/function in a flow file should be the Flow itself Flows should use the @project decorator (see workflow ) using a consistent name parameter throughout the project Flow steps should be as minimal as possible containing: Setup such as parsing and logging parameters or declaring context managers At most a few high-level function calls Any flow-level logic - e.g. merging artifacts Consider using type-annotations for the data artifacts Imports used within a step should always go within a step (at the top of it) Because metaflow has the ability to abstract infrastructure then different steps may run on different machines with different environments. from kuebiko.pipeline.sic.utils import * wouldn't work in a step - kuebiko isn't installed/packaged in the new conda environment. Avoiding saving data artifacts as dataframes (or other library-specific classes) favouring standard Python data-structures instead. Python data-structures do not impose the Pandas dependency on the downstream consumer of the data who may be working in an environment where Pandas isn't available (e.g. AWS lambda) or may have a different version of Pandas which when your artifact is loaded may subtly differ or fail to load. If dataframes are persisted as regular Python data-structure, the downstream consumer can still generate a dataframe if they want. When data is too large to be saved as a data artifact (not enough RAM or pickling/unpickling is inefficient) ... TODO Docstrings for steps: Always for start and end - their name cannot convey any information about what the step does For any step whose name cannot convey the essence of the step or for complex steps (steps that are more than one or two well documented function calls) Workflow \u00b6 Use local datastore during development Either pass --datastore=local when running a flow or add METAFLOW_DEFAULT_DATASTORE=local to .envrc to change the default datastore. This reduces the overhead to run a flow but means the results are local to your machine (the metadata of the run is still stored in an AWS RDS instance). Use datastore=s3 with --production to run your flow such that the artifacts can be seen by another user. Use the namespace relating to your @project name, e.g. if you're using @project(name=\"kuebiko\") for your flows, use metaflow.namespace(\"project:kuebiko\") Setting that in kuebiko/__init__.py means it gets set whenever you use a getter Using --metadata service (the default) means that your flow metadata will be visible via. the Metaflow UI at https://dap-tools.uk (more information will be available if using the S3 datastore too)! Things to watch out for \u00b6 Gotchas \u00b6 You cannot save functions as data artifacts, they get ignored. You can however save data artifacts with functions inside, e.g. self.artifacts = {\"key\": lambda _: \"value\"} Bugs \u00b6 Here are a bugs you may encounter: Using resume on a failed flow that uses metaflow.S3 may fail/be inconsistent https://github.com/Netflix/metaflow/issues/664 Metaflow's Conda decorators don't work with Python 3.9 https://github.com/Netflix/metaflow/issues/399 Poor errors \u00b6 Here are errors you may encounter which are uninformative: TypeError: Can't mix strings and bytes in path components - Use of metaflow.S3 with an incorrect metaflow profile","title":"Metaflow"},{"location":"guides/metaflow/#metaflow-guide","text":"","title":"Metaflow Guide"},{"location":"guides/metaflow/#recap-what-can-metaflow-do-for-you","text":"","title":"Recap: What can Metaflow do for you?"},{"location":"guides/metaflow/#data-versioning","text":"Each execution of a Flow has a unique run ID which data artifacts are stored under. The artifacts and metadata of each run can be fetched using the Metaflow client API .","title":"Data versioning"},{"location":"guides/metaflow/#scalable-computing","text":"Metaflow abstracts infrastructure making horizontally and vertically scalable cloud computing easily available to Data Scientists. In some (not all) cases it's as easy as adding --with batch to the end of the command to run a flow!","title":"Scalable computing"},{"location":"guides/metaflow/#sharing-results","text":"Data artifacts of runs can be stored and versioned not only on your machine but also to AWS S3, meaning you can share your results easily.","title":"Sharing results"},{"location":"guides/metaflow/#get-you-out-of-dependency-hell","text":"Ever run into a situation where one part of a project needs a package that conflicts with another? Metaflow allows you to specify dependencies at the flow level and automatically builds a Conda environment and runs your flow in it.","title":"Get you out of dependency hell"},{"location":"guides/metaflow/#make-your-pipelines-easier-to-understand","text":"Expressing a script as a DAG instead makes things a lot easier to understand. Metaflow also has commands such as show and output-dot which visualise this structure.","title":"Make your pipelines easier to understand"},{"location":"guides/metaflow/#checkpointing","text":"It's annoying when your code has gotten halfway through a pipeline only to tail. Metaflow automatically checkpoints the steps of your flow and lets you resume a run after fixing any issues, saving you having to re-run the whole pipeline or maintain your own checkpointing logic (that may be fraught with conflicts between different run parameters).","title":"Checkpointing"},{"location":"guides/metaflow/#suggested-practices","text":"","title":"Suggested practices"},{"location":"guides/metaflow/#writing-flows","text":"Naming a flow No need to add Flow as a suffix, the directory structure should speak to that Don't make names too generic, e.g. if the flow trains a topic model on Arxiv paper abstracts then call it something like ArxivAbstractTopicModel rather than just TopicModel Add a test mode If your flow takes more than a minute or two to run then you should always add a parameter test_mode = Parameter(\"test-mode\", ...) (by convention) which will run your flow in a test mode that runs quickly, e.g. fetch/run a subset of the data. This facilitates both efficient code review (a reviewer can check the test mode runs successfully without having to wait hours/days for the full flow to run) and automated testing. We recommend that by default self.test_mode is either: The logical not of current.is_production ( default=lambda _: not current.is_production ) True If a flow is run with --production ( current.is_production is True ) then test mode functionality should not be activated! Whilst often the logical not of one another, self.test_mode and current.is_production are two different concepts - self.test_mode and current.is_production may both be False when a user wants to run a flow without affecting other users. The only class/function in a flow file should be the Flow itself Flows should use the @project decorator (see workflow ) using a consistent name parameter throughout the project Flow steps should be as minimal as possible containing: Setup such as parsing and logging parameters or declaring context managers At most a few high-level function calls Any flow-level logic - e.g. merging artifacts Consider using type-annotations for the data artifacts Imports used within a step should always go within a step (at the top of it) Because metaflow has the ability to abstract infrastructure then different steps may run on different machines with different environments. from kuebiko.pipeline.sic.utils import * wouldn't work in a step - kuebiko isn't installed/packaged in the new conda environment. Avoiding saving data artifacts as dataframes (or other library-specific classes) favouring standard Python data-structures instead. Python data-structures do not impose the Pandas dependency on the downstream consumer of the data who may be working in an environment where Pandas isn't available (e.g. AWS lambda) or may have a different version of Pandas which when your artifact is loaded may subtly differ or fail to load. If dataframes are persisted as regular Python data-structure, the downstream consumer can still generate a dataframe if they want. When data is too large to be saved as a data artifact (not enough RAM or pickling/unpickling is inefficient) ... TODO Docstrings for steps: Always for start and end - their name cannot convey any information about what the step does For any step whose name cannot convey the essence of the step or for complex steps (steps that are more than one or two well documented function calls)","title":"Writing flows"},{"location":"guides/metaflow/#workflow","text":"Use local datastore during development Either pass --datastore=local when running a flow or add METAFLOW_DEFAULT_DATASTORE=local to .envrc to change the default datastore. This reduces the overhead to run a flow but means the results are local to your machine (the metadata of the run is still stored in an AWS RDS instance). Use datastore=s3 with --production to run your flow such that the artifacts can be seen by another user. Use the namespace relating to your @project name, e.g. if you're using @project(name=\"kuebiko\") for your flows, use metaflow.namespace(\"project:kuebiko\") Setting that in kuebiko/__init__.py means it gets set whenever you use a getter Using --metadata service (the default) means that your flow metadata will be visible via. the Metaflow UI at https://dap-tools.uk (more information will be available if using the S3 datastore too)!","title":"Workflow"},{"location":"guides/metaflow/#things-to-watch-out-for","text":"","title":"Things to watch out for"},{"location":"guides/metaflow/#gotchas","text":"You cannot save functions as data artifacts, they get ignored. You can however save data artifacts with functions inside, e.g. self.artifacts = {\"key\": lambda _: \"value\"}","title":"Gotchas"},{"location":"guides/metaflow/#bugs","text":"Here are a bugs you may encounter: Using resume on a failed flow that uses metaflow.S3 may fail/be inconsistent https://github.com/Netflix/metaflow/issues/664 Metaflow's Conda decorators don't work with Python 3.9 https://github.com/Netflix/metaflow/issues/399","title":"Bugs"},{"location":"guides/metaflow/#poor-errors","text":"Here are errors you may encounter which are uninformative: TypeError: Can't mix strings and bytes in path components - Use of metaflow.S3 with an incorrect metaflow profile","title":"Poor errors"}]}